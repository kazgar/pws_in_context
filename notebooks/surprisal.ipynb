{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\"\n",
    "\n",
    "targets = pd.read_csv(DATA_PATH / \"new_targets.csv\")\n",
    "sentences = pd.read_csv(DATA_PATH / \"upd_sent_n_comp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {}\n",
    "for col in targets.columns:\n",
    "    for tg in targets[col].tolist():\n",
    "        if isinstance(tg, str):\n",
    "            if col in target_dict.keys():\n",
    "                target_dict[col].append(tg.strip(\"*\"))\n",
    "            else:\n",
    "                target_dict[col] = [tg.strip(\"*\")]\n",
    "\n",
    "sentence_dict = {}\n",
    "for col in [col for col in sentences.columns if \"sent\" in col]:\n",
    "    for sent in sentences[col].tolist():\n",
    "        if isinstance(sent, str):\n",
    "            if col in sentence_dict.keys():\n",
    "                sentence_dict[col].append(sent.split(\" TARGET\")[0])\n",
    "            else:\n",
    "                sentence_dict[col] = [sent.split(\" TARGET\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sent_matching = {\"context\": [], \"target\": [], \"context_category\": [], \"target_category\": []}\n",
    "\n",
    "for target_cat in target_dict.keys():\n",
    "    for tg in target_dict[target_cat]:\n",
    "        for sentence_cat in sentence_dict.keys():\n",
    "            for sent in sentence_dict[sentence_cat]:\n",
    "                target_sent_matching[\"context\"].append(sent)\n",
    "                target_sent_matching[\"target\"].append(tg)\n",
    "                target_sent_matching[\"context_category\"].append(sentence_cat)\n",
    "                target_sent_matching[\"target_category\"].append(target_cat)\n",
    "\n",
    "target_sent_df = pd.DataFrame.from_dict(target_sent_matching)\n",
    "\n",
    "target_sent_df.to_csv(DATA_PATH / \"target_sent_matching.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Device used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substrings(S: str):\n",
    "    subs = set()\n",
    "    n = len(S)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n + 1):\n",
    "            subs.add(S[i:j])\n",
    "    return subs\n",
    "\n",
    "\n",
    "def token_string(tok: str, whitespace_char: str = \"Ä \"):\n",
    "    return tok.replace(whitespace_char, \" \")\n",
    "\n",
    "\n",
    "def build_lattice(S: str, token_strings: list[str]):\n",
    "    n = len(S)\n",
    "    lattice = [[] for _ in range(n + 1)]\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for tok in token_strings:\n",
    "            j = i - len(tok)\n",
    "            if j >= 0 and S[j:i] == tok:\n",
    "                lattice[i].append((j, tok))\n",
    "    return lattice\n",
    "\n",
    "\n",
    "# prob_cache = {}\n",
    "\n",
    "\n",
    "def get_probs(prefix: str, pruned_tokens: list[str], tok_to_id: dict):\n",
    "    # global prob_cache\n",
    "\n",
    "    if prefix in prob_cache:\n",
    "        return prob_cache[prefix]\n",
    "\n",
    "    ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(ids)\n",
    "    logits = out.logits[0, -1]\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    result = {}\n",
    "    for tok in pruned_tokens:\n",
    "        tid = tok_to_id[tok]\n",
    "        result[tok] = float(probs[tid].cpu())\n",
    "\n",
    "    prob_cache[prefix] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "def dp_prob(S: str, lattice: dict[str, list[str]], pruned_tokens: list[str], tok_to_id: dict):\n",
    "    n = len(S)\n",
    "    dp = [0.0] * (n + 1)\n",
    "    dp[0] = 1.0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j, tok in lattice[i]:\n",
    "            prefix = S[:j]\n",
    "            p = get_probs(prefix, pruned_tokens, tok_to_id)[tok]\n",
    "            contrib = dp[j] * p\n",
    "            dp[i] += contrib\n",
    "\n",
    "    return dp[n]\n",
    "\n",
    "\n",
    "def calculate_surprisal(context: str, target: str, tokenizer: AutoTokenizer):\n",
    "    global prob_cache\n",
    "\n",
    "    prob_cache = {}\n",
    "\n",
    "    sentence = context + \" \" + target\n",
    "    substring_set = all_substrings(sentence)\n",
    "\n",
    "    id2token = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "\n",
    "    token_strings = [token_string(tok) for tok in id2token]\n",
    "\n",
    "    pruned_tokens = []\n",
    "    pruned_token_ids = []\n",
    "\n",
    "    for tid, t in enumerate(token_strings):\n",
    "        if t in substring_set:\n",
    "            pruned_tokens.append(t)\n",
    "            pruned_token_ids.append(tid)\n",
    "\n",
    "    tok_to_id = dict(zip(pruned_tokens, pruned_token_ids))\n",
    "\n",
    "    lattice_context = build_lattice(context, pruned_tokens)\n",
    "    lattice_sentence = build_lattice(sentence, pruned_tokens)\n",
    "\n",
    "    P_context = dp_prob(context, lattice_context, pruned_tokens, tok_to_id)\n",
    "    P_sentence = dp_prob(sentence, lattice_sentence, pruned_tokens, tok_to_id)\n",
    "\n",
    "    P_cond = P_sentence / P_context\n",
    "    surprisal = -math.log(P_cond)\n",
    "    return surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sent_matching_df = pd.read_csv(DATA_PATH / \"target_sent_matching.csv\")\n",
    "\n",
    "surprisals = []\n",
    "\n",
    "for context, target in zip(\n",
    "    target_sent_matching_df[\"context\"].tolist(), target_sent_matching_df[\"target\"].tolist()\n",
    "):\n",
    "    surprisals.append(calculate_surprisal(context, target, tokenizer))\n",
    "\n",
    "target_sent_matching_df[\"surprisal\"] = surprisals\n",
    "\n",
    "target_sent_matching_df.to_csv(DATA_PATH / \"target_sent_surprisals.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
