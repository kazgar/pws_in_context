{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\"\n",
    "\n",
    "after_df = pd.read_csv(DATA_PATH / \"levenshtein_dist\" / \"lev_after_k=20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Device used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substrings(S: str):\n",
    "    subs = set()\n",
    "    n = len(S)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n + 1):\n",
    "            subs.add(S[i:j])\n",
    "    return subs\n",
    "\n",
    "\n",
    "def token_string(tok: str, whitespace_char: str = \"Ä \"):\n",
    "    return tok.replace(whitespace_char, \" \")\n",
    "\n",
    "\n",
    "def build_lattice(S: str, token_strings: list[str]):\n",
    "    n = len(S)\n",
    "    lattice = [[] for _ in range(n + 1)]\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for tok in token_strings:\n",
    "            j = i - len(tok)\n",
    "            if j >= 0 and S[j:i] == tok:\n",
    "                lattice[i].append((j, tok))\n",
    "    return lattice\n",
    "\n",
    "\n",
    "prob_cache = {}\n",
    "\n",
    "\n",
    "def get_probs(prefix: str, pruned_tokens: list[str], tok_to_id: dict):\n",
    "    # global prob_cache\n",
    "\n",
    "    if prefix in prob_cache:\n",
    "        return prob_cache[prefix]\n",
    "\n",
    "    ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(ids)\n",
    "    logits = out.logits[0, -1]\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    result = {}\n",
    "    print(\"here\")\n",
    "    print(pruned_tokens)\n",
    "    for tok in pruned_tokens:\n",
    "        tid = tok_to_id[tok]\n",
    "        result[tok] = float(probs[tid].cpu())\n",
    "\n",
    "    prob_cache[prefix] = result\n",
    "    print(\"here2\")\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def dp_prob(S: str, lattice: dict[str, list[str]], pruned_tokens: list[str], tok_to_id: dict):\n",
    "    n = len(S)\n",
    "    dp = [0.0] * (n + 1)\n",
    "    dp[0] = 1.0\n",
    "\n",
    "    print(\"it goes here\")\n",
    "    for i in range(1, n + 1):\n",
    "        for j, tok in lattice[i]:\n",
    "            print(\"does it go here\")\n",
    "            prefix = S[:j]\n",
    "            print(pruned_tokens)\n",
    "            p = get_probs(prefix, pruned_tokens, tok_to_id)[tok]\n",
    "            contrib = dp[j] * p\n",
    "            dp[i] += contrib\n",
    "\n",
    "    print(\"does this get returned\")\n",
    "    return dp[n]\n",
    "\n",
    "\n",
    "def calculate_surprisal(context: str, target: str, tokenizer: AutoTokenizer):\n",
    "    sentence = context + \" \" + target\n",
    "    substring_set = all_substrings(sentence)\n",
    "\n",
    "    id2token = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "\n",
    "    token_strings = [token_string(tok) for tok in id2token]\n",
    "\n",
    "    pruned_tokens = []\n",
    "    pruned_token_ids = []\n",
    "\n",
    "    for tid, t in enumerate(token_strings):\n",
    "        if t in substring_set:\n",
    "            pruned_tokens.append(t)\n",
    "            pruned_token_ids.append(tid)\n",
    "\n",
    "    tok_to_id = dict(zip(pruned_tokens, pruned_token_ids))\n",
    "\n",
    "    lattice_context = build_lattice(context, pruned_tokens)\n",
    "    lattice_sentence = build_lattice(sentence, pruned_tokens)\n",
    "\n",
    "    print(\"this executes\")\n",
    "    P_context = dp_prob(context, lattice_context, pruned_tokens, tok_to_id)\n",
    "    print(\"this doesnt\")\n",
    "    P_sentence = dp_prob(sentence, lattice_sentence, pruned_tokens, tok_to_id)\n",
    "\n",
    "    P_cond = P_sentence / P_context\n",
    "    surprisal = -math.log(P_cond)\n",
    "    return surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"I\"\n",
    "target = \"love\"\n",
    "\n",
    "print(calculate_surprisal(context, target, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
