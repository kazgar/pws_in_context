{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from pws_in_context.constants import PROJECT_ROOT\n",
    "\n",
    "top_k = 20\n",
    "DATA_PATH = PROJECT_ROOT / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Retrieve data and transform dataframes\n",
    "# =========================================\n",
    "association_data = \"association_data\"\n",
    "\n",
    "data_after_path = DATA_PATH / \"after_pws_in_context_data\" / association_data\n",
    "data_before_path = DATA_PATH / \"before_pws_in_context_data\" / association_data\n",
    "\n",
    "\n",
    "df_after = pd.concat([pd.read_csv(f) for f in data_after_path.iterdir()], axis=0).drop(\n",
    "    columns=[\"trial\", \"PROLIFIC_PID\"]\n",
    ")\n",
    "df_after[\"context\"] = df_after.apply(lambda row: row.sentence.split(f\" {row.target}\")[0], axis=1)\n",
    "\n",
    "df_before = pd.concat([pd.read_csv(f) for f in data_before_path.iterdir()], axis=0).drop(\n",
    "    columns=[\"trial\", \"PROLIFIC_PID\"]\n",
    ")\n",
    "df_before[\"context\"] = df_before.apply(lambda row: row.sentence.split(f\" {row.target}\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Custom supporting functions\n",
    "# ================================\n",
    "\n",
    "\n",
    "def top_k_predictions_for_context(\n",
    "    context: str, target: str, k: int = 5, tokenizer=None, model=None\n",
    ") -> float:\n",
    "    assert tokenizer is not None and model is not None, \"Provide tokenizer and model\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_logits = outputs.logits[:, -1, :]\n",
    "    _, topk_indices = torch.topk(last_logits, k=k, dim=-1)\n",
    "    topk_ids = topk_indices[0].tolist()\n",
    "\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for tok_id in topk_ids:\n",
    "        tok_str = tokenizer.decode([tok_id], skip_special_tokens=True)\n",
    "        tok_str_clean = tok_str.strip()\n",
    "        if tok_str_clean != \"\" and tok_str_clean.isalpha():\n",
    "            levenshtein = Levenshtein.distance(tok_str_clean, target) / max(\n",
    "                len(tok_str_clean), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    return sum(lev_dist_list) / len(lev_dist_list)\n",
    "\n",
    "\n",
    "def normalized_levenshtein_dist(target: str, associations: list[str]) -> float | None:\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for association in associations:\n",
    "        if isinstance(association, str):\n",
    "            levenshtein = Levenshtein.distance(association, target) / max(\n",
    "                len(association), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    if len(lev_dist_list) > 0:\n",
    "        return sum(lev_dist_list) / len(lev_dist_list)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_corr(df: pd.DataFrame, col1: str, col2: str) -> dict:\n",
    "    df = df[[col1, col2]].dropna()\n",
    "    pearson_r, pearson_p = pearsonr(df[col1].tolist(), df[col2].tolist())\n",
    "    spearman_r, spearman_p = spearmanr(df[col1].tolist(), df[col2].tolist())\n",
    "    kendall_tau, kendall_p = kendalltau(df[col1].tolist(), df[col2].tolist())\n",
    "    return {\n",
    "        \"pearson_r\": (pearson_r.item(), pearson_p.item()),\n",
    "        \"spearman_r\": (spearman_r.item(), spearman_p.item()),\n",
    "        \"kendall_tau\": (kendall_tau.item(), kendall_p.item()),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_scatter_with_corrs(\n",
    "    df: pd.DataFrame,\n",
    "    col1: str,\n",
    "    col2: str,\n",
    "    title: str,\n",
    "    cors: dict,\n",
    "    range: bool = False,\n",
    "    path: Path = None,\n",
    "    filename: str = None,\n",
    ") -> None:\n",
    "    p_r, p_a = cors[\"pearson_r\"][0], cors[\"pearson_r\"][1]\n",
    "    s_r, s_a = cors[\"spearman_r\"][0], cors[\"spearman_r\"][1]\n",
    "    k_tau, k_a = cors[\"kendall_tau\"][0], cors[\"kendall_tau\"][1]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"human_lev_dist\",\n",
    "        y=\"pred_lev_dist\",\n",
    "        color=\"class\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        title=title,\n",
    "        subtitle=f\"Pearson: {p_r:.2f}, p-value < 0.05: {p_a < 0.05} | \"\n",
    "        f\"Spearman: {s_r:.2f}, p-value < 0.05: {s_a < 0.05} | \"\n",
    "        f\"Kendall: {k_tau:.2f}, p-value < 0.05: {k_a < 0.05}\\n\",\n",
    "        labels={\n",
    "            \"human_lev_dist\": \"Levenshtein distance (human correlations - target)\",\n",
    "            \"pred_lev_dist\": \"Levenshtein distance (LLM predictions - target)\",\n",
    "        },\n",
    "        range_x=[0, 1] if range else None,\n",
    "        range_y=[0, 1] if range else None,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": fig.layout.title.text,\n",
    "            \"x\": 0.5,\n",
    "            \"xanchor\": \"center\",\n",
    "            \"yanchor\": \"top\",\n",
    "            \"font\": {\"size\": 24},\n",
    "        },\n",
    "        margin={\"t\": 150},\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_html(path / f\"{filename}.html\")\n",
    "    fig.write_image(path / f\"{filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Prepare model and tokenizer\n",
    "# ================================\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized levenshtein distance on LLM predictions and human data (association task after condition)\n",
    "df_after[\"pred_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: top_k_predictions_for_context(\n",
    "        context=row.context, target=row.target, k=top_k, tokenizer=tokenizer, model=model\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "association_cols_after = list(filter(lambda x: \"association\" in x, df_after.columns.tolist()))\n",
    "\n",
    "df_after[\"human_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: normalized_levenshtein_dist(\n",
    "        target=row.target,\n",
    "        associations=row[association_cols_after],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================================================\n",
    "# Calculate normalized levenshtein distance on LLM predictions and human data (association task before condition)\n",
    "# ===================================================================================================================\n",
    "df_before[\"pred_lev_dist\"] = df_before.progress_apply(\n",
    "    lambda row: top_k_predictions_for_context(\n",
    "        context=row.context, target=row.target, k=top_k, tokenizer=tokenizer, model=model\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "association_cols_before = list(filter(lambda x: \"association\" in x, df_before.columns.tolist()))\n",
    "\n",
    "df_before[\"human_lev_dist\"] = df_before.progress_apply(\n",
    "    lambda row: normalized_levenshtein_dist(\n",
    "        target=row.target,\n",
    "        associations=row[association_cols_before],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Save dataframes for further analysis\n",
    "# ===========================================\n",
    "LEVENSHTEIN_DATA_PATH = DATA_PATH / \"levenshtein_dist\"\n",
    "os.makedirs(LEVENSHTEIN_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before.to_csv(LEVENSHTEIN_DATA_PATH / f\"lev_before_k={top_k}.csv\", index=False)\n",
    "\n",
    "df_after.to_csv(LEVENSHTEIN_DATA_PATH / f\"lev_after_k={top_k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(LEVENSHTEIN_DATA_PATH / f\"lev_before_k={top_k}.csv\")\n",
    "df_after = pd.read_csv(LEVENSHTEIN_DATA_PATH / f\"lev_after_k={top_k}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Retrieve 'classes' of targets\n",
    "# ==============================\n",
    "targets_df = pd.read_csv(DATA_PATH / \"new_targets.csv\")\n",
    "\n",
    "\n",
    "def class_category(label):\n",
    "    if \"low\" in label:\n",
    "        return \"Low Prevalence Words\"\n",
    "    elif \"high\" in label:\n",
    "        return \"High Prevalence Words\"\n",
    "    else:\n",
    "        return \"Pseudowords\"\n",
    "\n",
    "\n",
    "class_map = {\n",
    "    item.strip(\"*\"): class_category(col)\n",
    "    for col in targets_df.columns\n",
    "    for item in targets_df[col].dropna()\n",
    "}\n",
    "\n",
    "df_before[\"class\"] = df_before[\"target\"].apply(lambda x: class_map[x])\n",
    "\n",
    "df_after[\"class\"] = df_after[\"target\"].apply(lambda x: class_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_PATH = PROJECT_ROOT / \"graphs\"\n",
    "os.makedirs(GRAPHS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plot after condition\n",
    "# =========================\n",
    "plot_scatter_with_corrs(\n",
    "    df=df_after,\n",
    "    col1=\"human_lev_dist\",\n",
    "    col2=\"pred_lev_dist\",\n",
    "    cors=calculate_corr(df_after, \"human_lev_dist\", \"pred_lev_dist\"),\n",
    "    title=\"Levenshtein distance (Human vs LLM against target) (after condition)\",\n",
    "    range=False,\n",
    "    path=GRAPHS_PATH,\n",
    "    filename=f\"lev_dist_after_k={top_k}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plot before condition\n",
    "# =========================\n",
    "plot_scatter_with_corrs(\n",
    "    df=df_before,\n",
    "    col1=\"human_lev_dist\",\n",
    "    col2=\"pred_lev_dist\",\n",
    "    cors=calculate_corr(df_before, \"human_lev_dist\", \"pred_lev_dist\"),\n",
    "    title=\"Levenshtein distance (Human vs LLM against target) (before condition)\",\n",
    "    range=False,\n",
    "    path=GRAPHS_PATH,\n",
    "    filename=f\"lev_dist_before_k={top_k}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
