{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "association_data = \"association_data\"\n",
    "\n",
    "data_after_path = DATA_PATH / \"big_pilot_at_after_processed_data\" / association_data\n",
    "data_before_path = DATA_PATH / \"big_pilot_at_before_processed_data\" / association_data\n",
    "\n",
    "\n",
    "df_after = pd.concat([pd.read_csv(f) for f in data_after_path.iterdir()], axis=0).drop(\n",
    "    columns=[\"trial\", \"PROLIFIC_PID\"]\n",
    ")\n",
    "\n",
    "df_after[\"context\"] = df_after.apply(lambda row: row.sentence.split(f\" {row.target}\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_predictions_for_context(\n",
    "    context: str, target: str, k: int = 5, tokenizer=None, model=None\n",
    "):\n",
    "    assert tokenizer is not None and model is not None, \"Provide tokenizer and model\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_logits = outputs.logits[:, -1, :]\n",
    "    _, topk_indices = torch.topk(last_logits, k=k, dim=-1)\n",
    "    topk_ids = topk_indices[0].tolist()\n",
    "\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for tok_id in topk_ids:\n",
    "        tok_str = tokenizer.decode([tok_id], skip_special_tokens=True)\n",
    "        tok_str_clean = tok_str.strip()\n",
    "        if tok_str_clean != \"\":\n",
    "            levenshtein = Levenshtein.distance(tok_str_clean, target) / max(\n",
    "                len(tok_str_clean), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    return sum(lev_dist_list) / len(lev_dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_levenshtein_dist(target: str, associations: list[str]):\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for association in associations:\n",
    "        if isinstance(association, str):\n",
    "            levenshtein = Levenshtein.distance(association, target) / max(\n",
    "                len(association), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    if len(lev_dist_list) > 0:\n",
    "        return sum(lev_dist_list) / len(lev_dist_list)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after[\"pred_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: top_k_predictions_for_context(\n",
    "        context=row.context, target=row.target, k=20, tokenizer=tokenizer, model=model\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "association_cols_after = list(filter(lambda x: \"association\" in x, df_after.columns.tolist()))\n",
    "\n",
    "df_after[\"human_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: normalized_levenshtein_dist(\n",
    "        target=row.target,\n",
    "        associations=row[association_cols_after],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
