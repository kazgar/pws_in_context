{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "top_k = 20\n",
    "DATA_PATH = PROJECT_ROOT / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data and transform dataframes\n",
    "association_data = \"association_data\"\n",
    "\n",
    "data_after_path = DATA_PATH / \"big_pilot_at_after_processed_data\" / association_data\n",
    "data_before_path = DATA_PATH / \"big_pilot_at_before_processed_data\" / association_data\n",
    "\n",
    "\n",
    "df_after = pd.concat([pd.read_csv(f) for f in data_after_path.iterdir()], axis=0).drop(\n",
    "    columns=[\"trial\", \"PROLIFIC_PID\"]\n",
    ")\n",
    "df_after[\"context\"] = df_after.apply(lambda row: row.sentence.split(f\" {row.target}\")[0], axis=1)\n",
    "\n",
    "df_before = pd.concat([pd.read_csv(f) for f in data_before_path.iterdir()], axis=0).drop(\n",
    "    columns=[\"trial\", \"PROLIFIC_PID\"]\n",
    ")\n",
    "df_before[\"context\"] = df_before.apply(lambda row: row.sentence.split(f\" {row.target}\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom supporting functions\n",
    "\n",
    "\n",
    "def top_k_predictions_for_context(\n",
    "    context: str, target: str, k: int = 5, tokenizer=None, model=None\n",
    ") -> float:\n",
    "    assert tokenizer is not None and model is not None, \"Provide tokenizer and model\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_logits = outputs.logits[:, -1, :]\n",
    "    _, topk_indices = torch.topk(last_logits, k=k, dim=-1)\n",
    "    topk_ids = topk_indices[0].tolist()\n",
    "\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for tok_id in topk_ids:\n",
    "        tok_str = tokenizer.decode([tok_id], skip_special_tokens=True)\n",
    "        tok_str_clean = tok_str.strip()\n",
    "        if tok_str_clean != \"\":\n",
    "            \"\"\"Check whether the token is only alphabetical characters\"\"\"\n",
    "            levenshtein = Levenshtein.distance(tok_str_clean, target) / max(\n",
    "                len(tok_str_clean), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    return sum(lev_dist_list) / len(lev_dist_list)\n",
    "\n",
    "\n",
    "def normalized_levenshtein_dist(target: str, associations: list[str]) -> float | None:\n",
    "    lev_dist_list = []\n",
    "\n",
    "    for association in associations:\n",
    "        if isinstance(association, str):\n",
    "            levenshtein = Levenshtein.distance(association, target) / max(\n",
    "                len(association), len(target)\n",
    "            )\n",
    "            lev_dist_list.append(levenshtein)\n",
    "\n",
    "    if len(lev_dist_list) > 0:\n",
    "        return sum(lev_dist_list) / len(lev_dist_list)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_corr(df: pd.DataFrame, col1: str, col2: str) -> dict:\n",
    "    df = df[[col1, col2]].dropna()\n",
    "    pearson_r, pearson_p = pearsonr(df[col1].tolist(), df[col2].tolist())\n",
    "    spearman_r, spearman_p = spearmanr(df[col1].tolist(), df[col2].tolist())\n",
    "    kendall_tau, kendall_p = kendalltau(df[col1].tolist(), df[col2].tolist())\n",
    "    return {\n",
    "        \"pearson_r\": (pearson_r.item(), pearson_p.item()),\n",
    "        \"spearman_r\": (spearman_r.item(), spearman_p.item()),\n",
    "        \"kendall_tau\": (kendall_tau.item(), kendall_p.item()),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_scatter_with_corrs(\n",
    "    df: pd.DataFrame, col1: str, col2: str, title: str, cors: dict, range: bool = False\n",
    ") -> None:\n",
    "    p_r, p_a = cors[\"pearson_r\"][0], cors[\"pearson_r\"][1]\n",
    "    s_r, s_a = cors[\"spearman_r\"][0], cors[\"spearman_r\"][1]\n",
    "    k_tau, k_a = cors[\"kendall_tau\"][0], cors[\"kendall_tau\"][1]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"human_lev_dist\",\n",
    "        y=\"pred_lev_dist\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        title=title,\n",
    "        subtitle=f\"Pearson: {p_r:.2f}, p-value < 0.05: {p_a < 0.05} | \"\n",
    "        f\"Spearman: {s_r:.2f}, p-value < 0.05: {s_a < 0.05} | \"\n",
    "        f\"Kendall: {k_tau:.2f}, p-value < 0.05: {k_a < 0.05}\\n\",\n",
    "        labels={\n",
    "            \"human_lev_dist\": \"Levenshtein distance (human correlations - target)\",\n",
    "            \"pred_lev_dist\": \"Levenshtein distance (LLM predictions - target)\",\n",
    "        },\n",
    "        range_x=[0, 1] if range else None,\n",
    "        range_y=[0, 1] if range else None,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": fig.layout.title.text,\n",
    "            \"x\": 0.5,\n",
    "            \"xanchor\": \"center\",\n",
    "            \"yanchor\": \"top\",\n",
    "            \"font\": {\"size\": 24},\n",
    "        },\n",
    "        margin={\"t\": 150},\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized levenshtein distance on LLM predictions and human data (association task after condition)\n",
    "df_after[\"pred_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: top_k_predictions_for_context(\n",
    "        context=row.context, target=row.target, k=top_k, tokenizer=tokenizer, model=model\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "association_cols_after = list(filter(lambda x: \"association\" in x, df_after.columns.tolist()))\n",
    "\n",
    "df_after[\"human_lev_dist\"] = df_after.progress_apply(\n",
    "    lambda row: normalized_levenshtein_dist(\n",
    "        target=row.target,\n",
    "        associations=row[association_cols_after],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized levenshtein distance on LLM predictions and human data (association task before condition)\n",
    "df_before[\"pred_lev_dist\"] = df_before.progress_apply(\n",
    "    lambda row: top_k_predictions_for_context(\n",
    "        context=row.context, target=row.target, k=top_k, tokenizer=tokenizer, model=model\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "association_cols_before = list(filter(lambda x: \"association\" in x, df_before.columns.tolist()))\n",
    "\n",
    "df_before[\"human_lev_dist\"] = df_before.progress_apply(\n",
    "    lambda row: normalized_levenshtein_dist(\n",
    "        target=row.target,\n",
    "        associations=row[association_cols_before],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes for further analysis\n",
    "LEVENSHTEIN_DATA_PATH = DATA_PATH / \"levenshtein_dist\"\n",
    "os.makedirs(LEVENSHTEIN_DATA_PATH, exist_ok=True)\n",
    "\n",
    "df_before.to_csv(LEVENSHTEIN_DATA_PATH / f\"lev_before_k={top_k}.csv\", index=False)\n",
    "\n",
    "df_after.to_csv(LEVENSHTEIN_DATA_PATH / f\"lev_after_k={top_k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_with_corrs(\n",
    "    df=df_after,\n",
    "    col1=\"human_lev_dist\",\n",
    "    col2=\"pred_lev_dist\",\n",
    "    cors=calculate_corr(df_after, \"human_lev_dist\", \"pred_lev_dist\"),\n",
    "    title=\"Levenshtein distance (Human vs LLM against target) (after condition)\",\n",
    "    range=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_with_corrs(\n",
    "    df=df_before,\n",
    "    col1=\"human_lev_dist\",\n",
    "    col2=\"pred_lev_dist\",\n",
    "    cors=calculate_corr(df_before, \"human_lev_dist\", \"pred_lev_dist\"),\n",
    "    title=\"Levenshtein distance (Human vs LLM against target) (before condition)\",\n",
    "    range=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
